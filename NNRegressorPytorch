import torch
import math
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Define the performance function
def performance_function(x1, x2):
    k = 7
    term1 = 3 + 0.1 * (x1 - x2)**2 - (x1 + x2)/(np.sqrt(2))
    term2 = 3 + 0.1 * (x1 - x2)**2 + (x1 + x2)/(np.sqrt(2))
    term3 = (x1 - x2) + k / (2**0.5)
    term4 = (x2 - x1) + k / (2**0.5)

    return min(term1, term2, term3, term4)

# Generate training data by randomly choosing N1 samples from S
N1 = 500
x1_train = np.random.normal(0, 1, size=N1)
x2_train = np.random.normal(0, 1, size=N1)
y_train = np.array([performance_function(x1, x2) for x1, x2 in zip(x1_train, x2_train)])

# Define the neural network
class PerformanceNet(nn.Module):
    def __init__(self):
        super(PerformanceNet, self).__init__()
        self.fc1 = nn.Linear(2, 5)
        self.fc2 = nn.Linear(5, 1)

    def forward(self, x):
        x = torch.tanh(self.fc1(x))
        x = torch.tanh(self.fc2(x))
        return x

# Convert training data to PyTorch tensors
x_t = torch.tensor(np.column_stack((x1_train, x2_train)), dtype=torch.float32)
y_train = y_train.reshape(-1, 1)
y_t = torch.tensor(y_train, dtype=torch.float32)

# Create an instance of the neural network
model = PerformanceNet()

# Define the loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.LBFGS(model.parameters(), lr=0.01)

# Train the neural network
num_epochs = 100000
for epoch in range(num_epochs):
    def closure():
        optimizer.zero_grad()
        outputs = model.forward(x_t)
        loss = criterion(outputs, y_t)
        loss.backward()
        return loss

    optimizer.step(closure)

n = 1000000
x1_test = np.random.normal(0, 1, size=n)
x2_test = np.random.normal(0, 1, size=n)
x_test = torch.tensor(np.column_stack((x1_test, x2_test)), dtype=torch.float32)

model.eval()  # Set the model to evaluation mode
with torch.no_grad():  # Disable gradient calculation
    y_pred = model(x_test)

y_pred = y_pred.numpy()
Pf_hat = np.sum(y_pred <= 0) / n
print(Pf_hat)
